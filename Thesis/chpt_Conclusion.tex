This thesis has presented a novel approach to mapping that combines
metric and topological information to build maps of medium to large
environments. In HTSLAM metric and topological representations are
equally important. Unlike many other hybrid approaches, HTSLAM does
not aim to build a global metric map with a single reference
frame. Instead, each local region has its own reference
frame. Relative poses of neighbouring regions are maintained in the
topological structure of a HTSLAM map. The proposed map structure
facilitates computation of relative poses between any two local
regions, effectively allowing a ``global'' reference frame to be
selected in any of the regions. A common problem for global SLAM is an
increasing uncertainty as the robot moves further away from the
origin. The HTSLAM map structure moves the origin into the current local
region, providing a more efficient representation of spatial
uncertainty.

HTSLAM uses particles to represent spatial uncertainty between local
regions. To the best of my knowledge there is no other published
research that combines topological mapping with particle filter metric
mapping. The main advantage of using particles is the ability to
efficiently represent multi-modal non-linear distributions. Approaches
that assume Gaussian uncertainty, suffer from accumulation of
approximation errors. The effect of linearisation becomes especially
significant in large scale environments. 

%Correlation between maps and transitions
It is generally impossible to partition a realistic environment into a
set of completely separate regions, some neighbouring regions are
likely to share common landmarks. Local maps cannot be truly
decorrelated. HTSLAM captures these inter region correlations in a
sample of map particles.


Loop closing is one of the difficult problems for SLAM. This thesis
presents an elegant solution that deals with the inherent ambiguities
of loop closing situation in a formal rigorous manner. HTSLAM does not
rely on some ``performance metrics'' that might be sensor/environment
specific, instead multiple mapping hypothesis are evaluated against
each other directly as part of a single particle filter. The hybrid
structure of HTSLAM makes it possible to delay loop closing until
there is sufficient evidence that the region has been visited
previously. In fact, loop closing can be delayed indefinitely, since
failure to close the loop does not result in an invalid map, only a
sub-optimal one.

The performance of HTSLAM has been tested on three distinct data sets:

\begin{itemize}
\item Laser range finder mounted on XR4000 robot, operating in an indoor
environment. Corners and doorways were used as landmarks.

\item Laser range finder mounted on a utility vehicle, operating in a
  park (the well-known Victoria Park data set \cite{VP_dataset}, made
  publicly available by the team from ACFR, Sydney). Tree trunks were
  used as landmarks.

\item Stereo cameras mounted on an XR4000 robotic platform, operating
  in an indoor environment. Vertical edges were used as landmarks.

\end{itemize}

%the results show...

%An estimate of the position of a region close to the current one is
%generally more certain than an estimate of the region far away.

\section{Future work}

There are a number of improvements to the current algorithms that I
believe can make mapping more robust and efficient. Experiments with
real data have shown that most failures occur after the transition
between local regions. This is because the process of transition
between regions generates a large proportion of weak particles. The
process of re-sampling will generally favour good particles and prune
out weak ones, however there is a danger that there will not be any
particles in the right place. A smarter sampling strategy that takes
into account recent observations, or overlap between local maps, can
provide a significant improvement over the current approach.

At the moment there is no obvious answer to how many particles are
needed to build a map of a particular environment. It would be
beneficial to automatically adjust the number of particles during
mapping, increasing the number of particles when the situation is
ambiguous (loop closing, or transition), and decreasing it when things
are going smoothly. Varying sample size has been used for the problem
of localisation \cite{KLDSampling}, it would be interesting to see if
such an approach can be applied to Rao-Blackwellised particle filters
as well.

Most environments contain static, semi-static and dynamic
features. Static features are always present, for example walls in the
indoor environment. Dynamic features include moving objects like
humans or other robots. Semi-static features are those that change
their position from time to time, a couch or a desk for
example. Traditional approaches to mapping assume a static world. While
these approaches can tolerate a small number of dynamic objects,
changes to the environment due to semi-static objects will render the
map unusable eventually, requiring a complete rebuild. I believe that
the HTSLAM map structure provides a good platform for a long term map
management system.




% LocalWords:  HTSLAM ACFR
